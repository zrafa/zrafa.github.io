<html>
<style>
@import url(http://fonts.googleapis.com/css?family=PT+Serif:400,700,400italic,700italic);

/* Set up basic page design */

@page { 
  /* size: letter; */
  size: a4;

  @footnotes {
    border-top: thin solid black; 
    border-clip: 100px;
    padding: 0;
    margin: 0.6em 0 0 0;
    padding: 0.3em 0 0 0;
  }
}

article { 
  /* columns: 2; */
  columns: 1;
  column-gap: 0.25in; 
  font: 11pt "PT Serif", serif;
  hyphens: auto;             /* turn on hyphenation */
  text-align: justify;       /* and justification */
  counter-reset: figure;
}

/* basic settings on commonly used elements */

html, body, div, header, p, blockquote, ul, ol, li, pre { margin: 0; padding: 0 }

p{
	margin-left:1em;
	margin-right:4em;

}
b { 
  /* background-color: #eeeefF; */
  font-style: italic;
  font-family: bold 12pt Lato, "Roboto Condensed", sans-serif;
}
li { margin-left: 1.5em }

header { 
  font: bold 12pt "PT Serif", serif; 
  margin: 1em 0 0.3em;
  page-break-after: avoid; break-after: avoid;
}

p { text-indent: 1.5em }
header + p { text-indent: 0 }
cite { font-style: normal }
pre { margin: 0.5em 0; padding: 0.3em; background: #eee }
ul { margin: 0.8em 0 }

/* sections-specific styling */

section.lead { 
  column-span: all; 
  text-align: center;
  margin: 2em 0;
  font-style: italic;
}

aside {
  background: #ddf;
  padding: 0.6em 1.3em 0.6em 1.3em;
  font-style: italic;
  font-family: Lato, "Roboto Condensed", sans-serif;
  width: 100%;
  box-sizing: border-box;
  float: none;
  margin-outside: 180mm;
  text-align: inside;
  hyphens: manual;
}



section.lead h1 { font: bold 14pt "PT Serif", serif; text-align: center } /* used for article title */

section.lead .authors {
  display: table;
  margin: 0 auto; 
}

section.lead .vcard {
  display: table-cell;
  text-align: center;
  font: 11pt "PT Serif", serif;
}

section.lead a {
  display: block;
  padding: 0 1em;
  color: black;
  text-decoration: none;
}

section.abstract header {
  text-align: center;
}

section.refs p {
  margin: 0.5em 0;
  text-indent: 0;
}

section.refs .author { 
  font-variant: small-caps;
}

section.refs ol, table ol {
  margin: 0; padding: 0;
}

section.refs li, table li {
  list-style-type: none;
  margin-left: 1.5em;
  text-indent: -1.5em;
}

table { border-collapse: collapse; margin: 1em 0; width: 100%; }

table td {
  border: thin solid black;
  padding: 0.2em;
}

/* counters */

section:first-of-type { counter-reset: section }
section.nonum { counter-reset: none }

section { counter-increment: section }
section.nonum { counter-increment: none }

header:before { content: counters(section, ".") " " }
section.nonum header:before { content: none }

/* footnotes */

::footnote-call {
  content: "[" counter(footnote, lower-latin) "]";
  font-size: 83%;
  vertical-align: super;
  line-height: none
}

::footnote-marker {
  content: "[" counter(footnote, lower-latin) "]";
  list-style-position: inside;
  margin: 0; padding: 0 0.3em 0 0;
}

.foot {
  float: prince-column-footnote;
  font-size: 90%;
  footnote-style-position: outside;
  margin: 0.3em 0 0 1.3em; padding: 0; text-indent: 0;
}

/* page floats */

.column-top { float: column-top; margin-bottom: 2em }
.column-bottom { float: column-bottom;  margin-top: 2em }
.top { float: top;  margin-bottom: 2em }
.bottom { float: bottom;  margin-top: 2em }
top figcaption, .bottom figcaption { margin-left: 2em; margin-right: 2em }

figure { 
  counter-increment: figure;
  font-size: 0.9em;
/*  min-width: 300px; */
  max-width: 640px;
/*  min-height: 300px; */
  max-height: 480px;
  text-align: left;
  width: 90%;
  float: center;
  margin: 0.4in;
}

figcaption:before { 
  font-weight: bold;
  content: "Figure " counter(figure) ": " 
}

@media screen {
  body {
    margin: 3em;
  }



p.date {
    text-align: right;
}

  article { 
    columns: 1;
    font: 16px/1.3 "PT Serif", serif;
    width: 100%;
  }
  .top, .bottom, .column-top, .column-bottom {
    float: right; 
    width: 55%;
    margin-right: -60%;
  }
  /* aside { float: none; width: auto; margin: 1em 0 } */
  aside { float: none; width: auto; margin: 0em 0em 0em 0em }
}


#customers {
    font-family: "Trebuchet MS", Arial, Helvetica, sans-serif;
    border-collapse: collapse;
    width: 100%;
}

#customers td, #customers th {
    border: 1px solid #ddd;
    padding: 8px;
  font: 10pt "PT Serif", serif;
}

#customers tr:nth-child(even){
background-color: #f2f2f2;
  font: 10pt "PT Serif", serif;
}

#customers tr:hover {
background-color: #ddd;
  font: 10pt "PT Serif", serif;
}

#customers th {
    /* font: 8pt */
    padding-top: 12px;
    padding-bottom: 12px;
    text-align: left;
    /* background-color: #4CAF50; */
    background-color: #4C50AF;
    color: white;
  font: 10pt "PT Serif", serif;
}
</style>

<body onload="makeref(); maketf();">
<article>


<section class=lead>
<h1>Memoria </h1>

<div class=authors>

<div class="vcard">
<i>
 <a class="url fn" href="http://www.pnr.iki.fi/cv.html">Rafael Ignacio Zurita</a>
 <a class="url org" href="http://www.aalto.fi">rafa@fi.uncoma.edu.ar</a>
</div>

<div class="vcard">
 <a class="url fn" href="http://www.pnr.iki.fi/cv.html">Rodolfo del Castillo</a>
 <a class="url org" href="http://www.usenix.org">rdc@fi.uncoma.edu.ar</a>
</i>
</div>

</div>

</section>

<section class='abstract nonum'>

<header>Resumen</header>

<!-- toc -->


</section>

<section>

<header> Introducción </header>
Para cualquier arquitectura de computadoras existen, por lo general, 
muchas implementaciones con diferentes precios y rendimiento.
El diseñador debe, en cada implementación, seleccionar una estructura
de computadora que cumpla con restricciones de costos y que, a su vez,
tenga el mejor rendimiento y confiabilidad posible. 
Esta estructura debe ser construida con tecnologías de componentes 
fácilmente disponibles y, tipicamente, debe poder ser modificable para
aceptar nuevas tecnologías.

<p><br><aside>This is a sample document to showcase page-based formatting. It contains a chapter from a <a href=http://en.wikibooks.org/>Wikibook</a> called <a href=http://en.wikibooks.org/wiki/Sensory_Systems>Sensory Systems</a>. None of the content has been changed in this article, but some content has been removed.
</aside>
<br>


<p>La memoria de la computadora es un repositorio para las intrucciones y para los datos. 
Las memorias están compuestas de dispositivos que pueden retener dos estados distinguibles, y que pueden utilizarse para representar los valores cero y uno.
Por lo tanto, cada uno de estos dispositivos (flip-flop semiconductor, magnetic core, o una moneda) puede representar un digito binario, o bit.

<p>Los bits en la memoria son organizados como un arreglo de unidades de información, donde cada unidad de información está compuesto de un número fijo de bits.
Además, cada unidad de información reside en una ubicación distinta en memoria, y cada ubicación es capaz de almacenar una unidad. Por lo que las memorias presentan dos caractaerísticas organizacionales:
- Cada unidad de información es de un mismo tamaño
- Una unidad de información tiene un número de dirección asociado, el cual puede ser referenciado univocamente.

<p>La unidad de información en memoria está caracterizada por dos cosas:

- Una dirección, la cual es su ubicación relativa en memoria.
- El contenido, el cual es el valor numérico que está fisicamente almacenado en esa ubicación particular en memoria.

<p>En las computadoras modernas la unidad direccionable mas chica
es siempre el byte (8 bits). Esto significa que cada unidad de almacenamiento
(en la memoria principal)
es un byte, y cada byte almacenado tiene una dirección unica.
Es importante recordar la diferencia entre la dirección i de un byte en memoria,
 y el contenido (el byte mismo) de esa dirección.

<p>En las arquitecturas de computadoras, existe tambien
el concepto de palabra (word), para definir a la unidad con la cual opera
la CPU (es decir, los operandos que puede direccionar y utilizar como
entrada en los calculos aritméticos y lógicos). La palabra (word) en maquinas 
modernas es de 32 y 64 bits de tamaño (por ejemplo, en intel x86 y MIPS). 
La arquitectura ARM es de 32 bits.

<p>En estas arquitecturas el bus de direcciones y datos es tambien organizado
por palabra. Esto significa que es posible leer (o escribir) una
palabra completa desde la memoria, en una unica instruccion de lectura 
(o escritura). Recuerde, sin embargo, que la unidad en las memorias
es el byte. Por lo tanto, en una arquitectura de 32bits (palabra de 32bits),
es posible leer una palabra completa en una instruccion. En memoria
principal, cada uno de los cuatro bytes leídos tiene una direccion diferente (cuatro direcciones consecutivas).

<p>El espacio de direcciones es el conjunto de todas las direcciones, o 
el número de unidades de datos distintas que un programa puede utilizar (referenciar).
El tamaño del espacio de direcciones es determinado por el número de bits
del bus de direcciones, utilizados para representar una dirección.
Una dirección es usualmente menor o igual que el tamaño de la palabra de
la arquitectura.
Una computadora intel x86 de 64bits tiene un espacio de direcciones a memoria
de 2^64 o 18446744073709551616 direcciones únicas.



</section>
<section>
<header>Evolucion</header>

<p>En la rápida evolución que han tenido las computadoras en las ultimas decadas,
el costo de la memoria ha sido el mayor factor en el precio general del 
sistema.
Aunque las memorias se hayan convertido tambien en componentes baratos
hay (y siempre habrán) tipos de memorias diferentes en cuanto
a rendimiento, capacidad de almacenamiento y costos.


<p>Debido a que la velocidad de la memoria tiene una influencia sustancial
en la velocidad de ejecución de la CPU, se tienen en cuenta diferentes tecnologías de memoria disponible al momento de diseñar la organización
general del sistema de memoria de un nuevo computador.
<div align="right" style="background:#fafaff;  padding-top: 0px; padding-bottom: 4px; font:10pt ; font-style:italic; margin-right: -10%;">jerarquia de memoria</div>
<p>El objetivo de utilizar diferentes tipos de memoria en un mismo sistema es conseguir el rendimiento de una memoria de gran velocidad al coste de una memoria de baja velocidad, basándose en el principio de cercanía de referencias.
Esta organización se denomina <b>jerarquía de memoria</b> de una computadora.



<p>Si el costo no es una restricción, el sistema de memoria entero podría
ser construido utilizando la memoria disponible mas rápida, como se hizo
para la computadora CRAY-1, y como suele hacerse en las supercomputadoras,
donde el costo no es un problema.
Para computadoras de bajo costo, esto es obviamente imposible.
De cualquier manera, una opción para computadoras personales y sistemas
embebidos es organizar el sistema de memoria dentro de una jerarquía
de niveles, donde cada nivel está compuesto de tecnologías de 
memorias diferentes (en cuanto a costo, capacidad y velocidad de acceso).
Esto puede hacerse debido al conocimiento existente, de las características
estadísticas de los patrones de acceso a memoria, que realizan 
programas típicos en ejecución. En la figura 1 se puede observar
un diagrama de esta jerarquía.

<!--figure class=column-top-->
<figure>
<a href="imagenes/Jerarquia_memoria.png"><img style="width: 100%" src="imagenes/Jerarquia_memoria.png"></a>
<figcaption>Jerarquía de memoria.</figcaption>
</figure>

<p> Un sistema de memoria típico en una computadora está compuesto de un 
número consecutivo de elementos de almacenamiento, usualmente bytes, en
donde cada uno tiene una única dirección física. Para leer el dato que
se encuentra en una ubicación de almacenamiento la CPU ubica la dirección
única en el bus de direcciones 
y recibe el 
dato desde la memoria, que retorna en el bus de datos, como
se muestra en la figura 11-2. Note que cada dirección univocamente
especifica una ubicación de memoria, por lo tanto, no se necesita
realizar una busqueda del dato.
</section>
<section>
<header> La aproximación de la tecnología mas rápida </header>

<p>Una manera de estructurar el sistema de memoria total es implementar un 
segmento de la memoria con la tecnología mas rápida disponible.
Ejemplificando, suponga que en un sistema existen 64Kbytes de memoria
más rápida posible, en la cual el usuario puede completar con datos y código
ejecutable. El usuario entonces ubicaría allí (al momento de cargar el programa) el código y los datos mas 
criticos, en cuanto a cantidad de accesos y tiempo de ejecución.
Esto le permite predecir el rendimiento y tiempo de ejecución con exactitud.
De cualquier manera, este método beneficia unicamente a sistemas
de propósitos específicos (ejemplo: sistemas embebidos), pero 
es improbable que beneficie a un sistema de tiempo compartido
(time-sharing system), en el cual la memoria física es asignada y reasignada
entre los usuarios durante distintos momentos (computadoras de propósito
general).
Otra desventaja de esta aproximación es que el programador debe
conocer la arquitectura de la memoria física para utilizarla correctamente.
</section>
<section>

<header> Memoria Caché </header>

<p>
La tecnología más común utilizada en una jerarquía de memoria de una computadora es la <b><i>memoria caché</i></b> (pronunciado "cash", del francés "cacher" que siginifica "ocultar/esconder").
Nota al pie de pagina: está oculta desde el punto de vista del programador, aparece como parte del espacio de memoria del sistema.

<p>La <b><i>memoria caché</i></b> es una memoria pequeña, costosa y de muy alta velocidad. Se encuentra ubicada junto a la CPU y mantiene las instrucciones y datos mas recientemente utilizados. Cuando un programa realiza un requisito a memoria, la CPU verifica primero si el dato está en la caché. Si está, entonces el dato es traído rapidamente sin necesidad de acceder la memoria principal. El objetivo primordial es reducir el tiempo medio de acceso a los datos de la memoria principal. En la figura 2 puede observarse una arquitectura que utiliza tres niveles de memoria caché.




<!--figure class=column-top-->
<figure>
<a href="imagenes/cachehierarchy2.svg"><embed style="width: 100%" src="imagenes/cachehierarchy2.svg"></a>
<figcaption>Esquema simplificado de una moderna arquitectura Intel x86, con cuatro nucleos y 3 niveles de caché dentro del chip del procesador.</figcaption>
</figure>


 <p>La memoria caché se puede comprender, en términos cotidianos, por su analogía con un agenda o cuaderno utilizado para registrar números de teléfono. Una guía telefónica completa (paginas blancas) contiene cientos de miles de números de teléfono, y nadie lleva un directorio telefónico todo el tiempo. Sin embargo, la mayoría de la gente tiene una agenda o cuaderno de notas con un centenar de números de teléfono de gente conocida. Aunque la fracción de todos los números de teléfono guardados en un cuaderno de alguien podría ser inferior al 0,01% del total de la guía telefónica, la probabilidad de que su próxima llamada sea a un número del cuaderno es muy alta. ¿Por qué? Porque la gente tiende a llamar a amigos y colegas con mucha frecuencia.

 <p>Un sistema de memoria con caché opera exactamente con el mismo principio del cuaderno de notas, encontrando la información que la CPU requiere frecuentemente en la memoria caché, en vez de en la memoria principal, la cual es  mucho más lenta. Desafortunadamente, a diferencia del cuaderno personal, la computadora no puede saber, a priori, qué datos serán posiblemente accedidos. Las caches de computadora funcionan según un principio de aprendizaje. Por experiencia aprenden qué datos se utilizan con más frecuencia y luego lo transfieren a la memoria caché.

 <p>La estructura general de una memoria caché se puede observar en la Figura 1. Un bloque de memoria caché se encuentra conectado a los buses de dirección y datos, en paralelo con la memoria principal mucho más grande. Tenga en cuenta que la afirmación de paralelo significa que los datos en la caché también se mantienen en la memoria principal. Para volver a la analogía con el cuaderno telefónico, escribir el número de un amigo en el cuaderno no elimina el número del directorio.

<!--figure class=column-top-->
<figure>
<a href="imagenes/cachehierarchy.svg"><embed style="width: 100%" src="imagenes/cachehierarchy.svg"></a>
<figcaption>Diagrama de bloques de una memoria caché general.</figcaption>
</figure>



La memoria caché requiere de un controlador para determinar si los datos que están siendo referenciados actualmente por la CPU residen en la memoria caché o si deben obtenerse de la memoria principal. Cuando el controlador de memoria caché obtiene la dirección del bus, se aplica la dirección actual al controlador de memoria caché, el controlador devuelve una señal llamada hit, que se afirma si los datos están actualmente en la memoria caché. Antes de ver cómo se organizan las memorias caché, demostraremos su efecto en el rendimiento de un sistema.


<p>El funcionamiento de la memoria caché se basa en el mismo principio que el
cuaderno con números de teléfono, ya que
el procesador no ejecuta instrucciones aleatoriamente. 
Debido a la naturaleza de los programas y sus estructuras de datos correspondientes, 
los datos requeridos por el procesador suelen estar muy agrupados en la memoria.
Frecuentemente se acceden en forma secuencial
(por ejemplo, cuando la CPU accede a las instrucciones del programa)
o desde zonas cercanas a direcciones recientemente
accedidas (cuado por ejemplo se procesa un arreglo o matrix de información).


<p>La observación del comportamiento de los programas muestra que las referencias a memoria realizadas
en un intervalo corto de tiempo tienden a utilizar una pequeña fracción
del total de la memoria.
Este comportamientno de acceso ha sido llamado <b><i>principio de localidad
de las referencias</i></b>[DENN68], o simplemente principio de localidad.
<p>Existen al menos dos tipos básicos de localidad: localidad espacial y temporal.
La localidad temporal se refiere a la reutilización de datos especificos,
dentro de un tiempo relativamente corto. Cuando un programa accede a la
ubicación en memoria de un dato o instrucción es bastante probable que vuelva a
acceder a la misma ubicación pronto. La localidad espacial, en cambio, se refiere
a la utilización de datos en ubicaciones de memoria cercanas a los elementos
accedidos recientemente. Si un programa accede a un dato o instruccion
en memoria es
altamente probable que tambien referencie a datos o instrucciones alojados
en direcciones proximas.

<p>Este fenómeno demostrado ha dado una base estadística para el diseño
de la estructura de la memoria de un computador basado en una jerarquía de memoria.

<p>En este caso, si la caché puede mantener suficientes datos para evitar
un gran número de referencias a la memoria principal, la velocidad
de ejecución incrementa substancialmente, y la utilizacion de la memoria
principal y del bus del sistema es también reducida beneficiosamente.
El objetivo es lograr una gran capacidad de memoria (esto se consigue
con memorias baratas y lentas) pero que puedan accederse con alta 
velocidad (a través de la utilización
de pequeñas memorias cachés entre la CPU y la memoria principal).

<section>
<header> Tiempo medio de acceso </header>


<p>El parámetro principal de un sistema de caché es su <b><i>tasa de aciertos (hit ratio)</i></b>, h, que define la proporción de aciertos a todos los accesos. La tasa 
de aciertos es determinada a través de observaciones estadísticas del funcionamiento de un sistema real, y no puede calcularse facilmente. 

<p>Además, la proporción de aciertos depende de la naturaleza real de los programas que se están ejecutando. Es posible tener algunos programas con tasas de aciertos muy altas y otros muy bajas. Afortunadamente, el efecto de la localidad de las referencias provoca que la proporción de aciertos es, generalmente, del 98%. Antes de calcular el efecto de una memoria caché en el rendimiento de un procesador, necesitamos introducir algunos términos.


<p>Si un dato es leído o escrito k veces en un intervalo corto de tiempo
entonces la computadora necesita 1 referencia a memoria principal, y 
k - 1 referencias a memoria caché. Cuanto mas grande sea k mejor
será el rendimiento del sistema.
Es posible formalizar este cálculo utilizando el tiempo de acceso a caché,
que llamaremos c, y el tiempo de acceso a memoria principal, llamado aquí m.
h es la tasa de aciertos, la cual es la fracción de las referencias
que están disponibles en caché. 
<p> h = (k -1)/k. Con estas variables y definiciones el tiempo medio
de acceso = c + (1 - h) m
<p> Si h tiende a 1 entonces todas las referencias pueden
ser resueltas por la caché y el tiempo de acceso se aproxima a c. Por
otro lado, si h tiende a 0, cada referencia debe ser satisfecha
por la memoria principal y el tiempo de acceso se aproxima a c + m
(ya que para cada acceso primero se intenta resolver la referencia 
en la caché, pero al haber un fallo, se accede a memoria principal).
Una estrategia de mejora para este peor caso es realizar el acceso
a memoria principal en paralelo con la referencia a cache', pero 
esto requiere de mecanismos para descartar o detener la lectura
a memoria si ocurre un cache' hit.


<p>En la práctica la situación real no es tan simple como las ecuaciones sugieren.
Las computadoras son dispositivos sincronizados por relojes, y funcionan a la velocidad determinada por el reloj.
Consecuentemente, los accesos a memoria operan en uno o más ciclos de reloj. 
Suponga, hipotéticamente, que un procesador accede
a memoria principal en un ciclo de reloj, entonces agregando una caché no hará al sistema más rápido.



<p>El efecto del rendimiento de una memoria caché de una computadora depende de muchos factores, incluyendo la
manera en que la caché está organizada, y la forma en el cual los datos son escritos a memoria principal
cuando se realizan escrituras. 

<p><br><aside>La memoria caché puede mejorar el rendimiento de una computadora dramaticamente, a cambio de un costo adicional relativamente bajo.
</aside>
<br>



<p>El tiempo de acceso a cada nivel de la jerarquía de memoria puede verse reflejado en la Tabla 1, para 
una computadora de arquitectura moderna (en este caso de un procesaodr Core i7, de arquitectura Intel X86). Cada core puede ejecutar una instruccion cada 2 nanosegundos.


 <table id="customers" style="width:100%">
<caption>Tabla 1. Tiempos de acceso a memoria en una sistema moderno (Core i7 Xeon 5500 Series)</caption>
  <tr>
    <th>Memoria - Nivel</th>
    <th>Nro de ciclos (aprox)</th>
    <th>Tiempo de acceso (aprox)</th>
  </tr>
  <tr>
	<td>L1 CACHE hit</td> <td>~4 cycles</td> <td>~2.0 ns</td>
  </tr>
  <tr>
	<td>L2 CACHE hit</td> <td>~10 cycles</td> <td>~3.0 ns</td>
  </tr>
  <tr>
	<td>L3 CACHE hit, line unshared</td> <td>~40 cycles</td> <td>~12.0 ns</td>
  </tr>
  <tr>
	<td>L3 CACHE hit, shared line in another core</td> <td>~65 cycles</td> <td>~19.5 ns</td>
  </tr>
  <tr>
	<td>L3 CACHE hit, modified in another core</td> <td>~75 cycles</td> <td>~22.5 ns</td>
  </tr>
  <tr>
	<td>remote L3 CACHE</td> <td>~100-300 cycles</td> <td>~30 ns</td>
  </tr>
  <tr>
	<td>DRAM</td> <td>~200 cycles</td> <td>~60 ns</td>
  </tr>
</table> 


<p> Si la caché puede almacenar suficienten datos para evitar un gran 
numero de referencias a memoria principal (lenta) entonces no sólo
la velocidad de ejecución incrementa, sino que tambien se reduce el uso
del bus del sistema y memoria.


</section>
<section>



<header>Organizacion interna de la memoria caché </header>
<p>
La estructura interna y el funcionamiento de una memoria caché 
describen dónde debe colocarse un bloque de memoria principal 
cuando se almacena en la caché. 


<p>Hay al menos tres maneras de organizar una memoria caché, cada una con su propia complejidad y rendimiento:
caché de mapeo directo (direct-mapped cache), caché asociativa (associative cache), y caché asociativa por conjunto de n-vias (n-vias set associative cache).

<section>
<header>Caché de mapeo directo</header>



<p>La forma más sencilla de organizar una memoria caché es utilizar un mapeo directo, que se basa en un algoritmo simple: asignar el bloque de datos i de la memoria principal al bloque de datos i en la memoria caché. 


<p>La figura 4 ilustra la estructura de una memoria caché simple de mapeo directo. 
La memoria está compuesta de 32 palabras y se accede mediante un bus de direcciones de 5 bits que lo conecta a la CPU y caché.
El espacio de memoria ha sido divido en conjuntos y los conjuntos en líneas. 
<p>La caché contiene "entradas" llamadas comunmente <b><i>líneas de caché (cache line)</i></b>. En este ejemplo particular cada línea de caché mantiene un bloque datos pequeño, formado por dos palabras consecutivas.

<p>Cada dirección es de 5 bits, y está compuesta por un campo de 2 bits para indicar el conjunto, un campo de 2 bits para indicar la línea de caché, y un campo de selección de palabra de 1 bit. La memoria caché tiene 4 líneas de dos palabras cada una. Cuando el procesador genera una dirección, se accede a la línea apropiada en la caché. Por ejemplo, si el procesador genera la dirección de 5 bits 01010, se accede a la línea 2 del conjunto 2.



<!--figure class=column-top-->
<figure>
<a href="imagenes/directo1.gif"><embed style="width: 70%" src="imagenes/directo1.gif"></a>
<figcaption>
La memoria caché simple de mapeo directo.</figcaption>
</figure>


 

<p>Un vistazo a la figura 4 revela que hay cuatro líneas número dos posibles: una línea 2 en el conjunto 0, una línea 2 en el conjunto 1, una línea 2 en el conjunto 2 y una línea 2 en el conjunto 3. En este ejemplo, el procesador accedió a la línea 2 en el conjunto 2. La pregunta obvia es, '¿cómo sabe el sistema si la línea 2 accedida en la caché es la línea 2 del conjunto 2 en la memoria principal?'

<p>La Figura 5 muestra que existe tambien una <b><i>etiqueta (tag)</i></b> asociada a cada línea en la memoria caché, que determina a qué conjunto pertenece esa línea. 
De esta manera es posible confirmar que la línea en la caché pertenece o no al conjunto especificado en la direcciión originada por el procesador.
Cuando el procesador accede a la línea 2, la etiqueta que pertenece a la línea 2 de la memoria caché se envía a un comparador. Al mismo tiempo, el campo del conjunto de la dirección originada por el procesador también se envía al comparador. Si son iguales, la línea en la caché es la línea deseada y se produce un <b><i>acierto (hit)</i></b>. 

<!--figure class=column-top-->
<figure>
<a href="imagenes/directo2.gif"><embed style="width: 70%" src="imagenes/directo2.gif"></a>
<figcaption>
Resolución del conjunto de cada línea en una caché de mapeo directo.
</figure>

<p>Si las etiquetas (conjuntos) no son los iguales, se produce un <b><i>fallo (miss)</i></b> y se debe actualizar la memoria caché.
La antigua línea 2 del conjunto 1 es descartada o reescrita de nuevo a la memoria principal,
dependiendo de cómo está organizada la actualización de la memoria principal.


<p>La figura 6 muestra la estructura de un sistema de memoria caché de mapeo directo.
La caché es simplemente un bloque de memoria de lectura / escritura de acceso aleatorio de muy alta velocidad.
Tambien incorpora un comparador y una memoria rápida para las etiquetas.
El bus de direcciones contiene la dirección de entrada a la caché el cuál es utilizada para acceder a una única ubicación de línea de caché. La etiqueta en esa línea es entonces comparada con el valor del conjunto presente como parte de la dirección de entrada proveniente del bus de direcciones. Si la etiqueta concuerda con el número de conjunto de la dirección la <b></i>señal de acierto (hit signal)</i></b> es activada.

<p>Como muestra la Figura 6, la memoria de etiquetas de la caché no es más que una simple memoria de acceso aleatorio de alta velocidad con un comparador de datos incorporado. Algunos de los principales fabricantes de semiconductores han implementado esta memoria de etiquetas de caché de un solo chip.

<!--figure class=column-top-->
<figure>
<a href="imagenes/directo3.gif"><embed style="width: 70%" src="imagenes/directo3.gif"></a>
<figcaption>
Implementación de caché de mapeo directo.
</figure>


 

<p>La ventaja de la memoria caché de mapeo directo es casi evidente. Esta caché es un dispositivo 
ampliamente disponible que, aparte de su velocidad, no son más complejos que cualquier otro circuito integrado.
Además, esta caché no requiere ningún algoritmo de sustitución de línea. 
Si se accede a la línea x en el conjunto y se produce un fallo de caché, la línea x del conjunto y se carga de la memoria principal y se almacena en la línea x de la memoria caché. Por lo tanto, no hay mecanismos de decisión que deba seleccionar cuál línea de caché debe ser reemplazada cada vez que una nueva línea es cargada desde la memoria principal.

<p>Otra ventaja importante es su paralelismo inherente. Debido a que la caché y la memoria de etiquetas son independientes, ambas se pueden acceder simultáneamente. Una vez que la etiqueta ha coincidido y se ha producido un acierto, los datos presentados por la caché también serán válidos (suponiendo que la caché y la memoria de etiquetas tengan tiempos de acceso aproximadamente iguales).

<p>La desventaja de esta caché es casi un corolario de su ventaja. Un caché con n líneas tiene una restricción: en cualquier instante puede contener sólo una línea numerada x. Por lo tanto, no puede mantener una línea x del conjunto p y una línea x del conjunto q. Esta restricción existe porque hay un único bloque de datos en la caché para cada una de las lineas posibles. Para ejemplificar, observe el siguiente fragmento de código que llama a dos subrutinas en un bucle:

<br> <br>
<br> REPETIR
<br>      LLAMAR Get_data
<br>      LLAMAR Compare
<br> HASTA coincidencia o fin_de_los_datos
<br> <br> 


<p>Suponga que en la versión compilada de este código parte de la subrutina Get_data está en el conjunto x, línea y; y que parte de la subrutina Compare está en el conjunto z, línea y.
Debido a que una caché de mapeo directo puede contener sólo una línea y a la vez, la trama correspondiente a la línea y debe recargarse dos veces para cada conjunto a través del bucle.
El rendimiento de la caché para este caso es muy bajo (fallos de caché frecuentes).

<p>Supongamos ahora que una memoria caché de mapeo directo está casi vacía, y que la mayoría de sus líneas aún no se han cargado con datos. Sin embargo, las únicas líneas con datos válidos deben reemplazarse con frecuencia 
debido a que los accesos a memoria son direcciones con diferentes números de conjuntos pero misma línea.
Este caso también tendrá un rendimiento pobre (fallos de caché frecuentes). 
<p>A pesar de los ejemplos anteriores, mediciones estadísticas de programas reales indican que el rendimiento bajo del peor caso no tiene un impacto significativo en su rendimiento general.
Además, para ciertos casos, puede trabajar mejor que una caché totalmente asociativa, como veremos luego.
<p>La caché de mapeo directo es muy popular debido a su bajo costo de implementación y alta velocidad.

<p> CUADRO: de mejor caso que asociativa


</section>



<section>
<header>Caché asociativa</header>
<p>Una excelente forma de organizar una memoria caché que no presenta las limitaciones de la caché de mapeo 
directo se describe en la Figura 7, la cual se denomina memoria caché asociativa.
Este tipo de caché no tiene restricciones en cuanto a qué datos puede contener cada entrada.
En otras palabras, cada entrada en esta caché puede almacenar cualquier bloque (línea) de la memoria principal.

<p>Una dirección del procesador se divide en tres campos: la etiqueta, la línea y la palabra. 
Al igual que la memoria caché de mapeo directo, la unidad más pequeña de datos transferido 
dentro y fuera de la caché es la línea.
A diferencia de la caché de mapeo directo, no existe una relación
entre la ubicación de las líneas en la memoria principal con respecto a la ubicación
de las líneas en la caché.
La línea p en memoria principal se puede almacenar en la entrada q de la caché,
sin restricciones sobre los valores de p y q. 

<p>Como ejemplo, considere un sistema con 1 Mbyte de memoria principal y 64Kbytes
de caché asociativa. Si el tamaño de una línea es de cuatro palabras de 32 bits 
(es decir, 16 bytes), la memoria principal está compuesta de 220/16 = 64K líneas
y la caché está compuesta de 216/16 = 4096 líneas. 
Debido a que una caché asociativa permite que cada una de sus entradas
se cargue con cualquier línea de la memoria principal, cada entrada
puede contener cualquier de  
las 64K líneas posibles de la memoria principal. 
Para identificar de manera unívoca a qué línea de la memoria principal 
pertenecen las palabras almacenadas en cada entrada de la caché se utiliza,
en este caso, una etiqueta de 16bits asociada a cada entrada.

<!--figure class=column-top-->
<figure>
<a href="imagenes/asociativa1.gif"><embed style="width: 70%" src="imagenes/asociativa1.gif"></a>
<figcaption>
Caché asociativa.
</figure>


<p>Cuando el procesador genera una dirección, los bits de la misma seleccionan una ubicación tanto en la memoria principal como en la memoria caché.
Los bits que identifican la línea en la dirección no se pueden utilizar para direccionar 
una entrada en la memoria caché (a diferencia de la memoria caché de mapeo directa).
Como la caché asociativa puede almacenar cualquier línea de la memoria de 64K en una de sus
entradas, requiere una etiqueta de 16 bits (suponiendo que hay 216 líneas posibles
en la memoria principal) para cada una de sus entradas.
Más importante aún, ya que las entradas de la caché (es decir, líneas) no están ordenadas,
las etiquetas no están ordenadas y no se pueden almacenar en una tabla de búsqueda simple
como la memoria caché de mapeo directo.
En otras palabras, cuando la CPU requiere acceder a la línea i, el dato puede 
estar en cualquier entrada de la caché o puede no estar en la caché.

<p>La caché asociativa emplea un tipo especial de memoria llamada memoria asociativa.
Una memoria asociativa tiene una entrada de n bits (proveniente del bus de direccion),
 pero no necesariamente 2n ubicaciones
internas únicas. La entrada de dirección de n bits es una etiqueta que es
comparada con la etiqueta de cada una de sus entradas simultáneamente.

<p>Si la etiqueta de entrada coincide con una etiqueta almacenada en la caché,
los datos asociados con esa ubicación se presentan como salida. De lo contrario, la caché asociativa produce un <b><i>fallo de caché (caché miss)</i></b>.
Una memoria asociativa no es direccionada de la misma manera que la memoria principal.
La memoria principal de la computadora requiere la dirección explícita de una localización, mientras que una memoria asociativa es accedida preguntando, "¿tiene este elemento almacenado en alguna parte?"

<p>Las memorias caché asociativas son eficientes porque la etiqueta que especifica la línea que se 
quiere obtener se compara simultáneamente con la etiqueta de cada entrada de la memoria caché.
En otras palabras, se accede a todas las ubicaciones al mismo tiempo. 
Desafortunadamente las memorias asociativas, aunque pequeñas, son muy costosas,
y cachés asociativas grandes no existen.
Además, una vez que la caché asociativa está llena, sólo se puede
introducir una nueva línea sobrescribiendo una existente.
Por lo tanto, las cachés asociativas deben utilizar alguna política de sustitución
para la entrada (línea) siendo descartada.

<p>Las memorias caché totalmente asociativas no son prácticas, debido a su alto costo.
Sin embargo, la mayoría de las computadoras emplean un tipo de caché organizada 
de maneria mixta, utilizando algunos mecanismos de las cachés de mapeo directo y algunos de las cachés totalmente asociativas.
Este sistema combinado se llama <b><i>caché asociativa por conjuntos</i></b>.

<p>Una caché asociativa por conjuntos está compuesta varias cachés de mapeo directo operados en paralelo.
La disposición más simple se denomina <b><i>caché asociativa por conjunto de 2 vías</i></b> y consta de dos 
memorias caché de mapeo directo. Cada línea en el sistema de caché está duplicado; Por ejemplo, hay dos líneas números 5 en la memoria caché. En consecuencia, ahora es posible almacenar dos líneas número 5, una línea 5 del conjunto x y una línea 5 del conjunto y.

<p>La Figura 8 es una descripción de una <b><i>caché asociativa por conjunto de 4 vías</i></b>, la cuás es muy utilizada.
Cuando el procesador presenta una dirección de memoria a ser accedida la línea apropiada en cada una de las cuatro cachés de mapeo directo es accedida simultáneamente.
Debido a que hay cuatro líneas, se utiliza un comparador sensicillo 
para determinar cuál (si presente) de las líneas en las cachés es la apropiada para
recuperar los datos. En la figura 8, la salida de acierto (hit) de cada caché de mapeo directo
es entrada a una puerta OR el cual genera un acierto (hit) si alguna de las
cachés generó un acierto.

<!--figure class=column-top-->
<figure>
<a href="imagenes/asociativoporconjuntos.gif"><embed style="width: 70%" src="imagenes/asociativoporconjuntos.gif"></a>
<figcaption>
Caché asociativa por conjuntos de 4 vías.
</figure>

</section>
</section>
<section>
<header>Estrategias en ciclos de escritura</header>

Además de seleccionar una organización y política de reemplazo para el sistema de caché 
el diseñador tiene que considerar cómo se deben tratar los ciclos de escritura. 

<p>Una tecnica común es modificar el dato en la entrada de la caché, sin modificar la memoria principal. Este método es llamado estrategia de <b><i>escritura diferida (write-back)</i></b>,
y reduce el número de escrituras a memoria principal en muchas escrituras consecutivas de la misma entrada.
Un bit extra para cada entrada está presente en la caché, y se utiliza para indicar si la línea en la caché fue modificada.
El hardware es más complejo en este caso, ya que se debe verificar este bit para conocer cuándo se debe escribir a memoria principal. Cuando una entrada de la caché debe ser reemplazada debido a un fallo de caché (cache miss) 
el bit de modificación es verificado. Si los datos en la línea a ser reemplazada difiere de los datos en memoria principal (esto es, el bit indica que se encuentra modificada), un ciclo de escritura es realizado primero desde la caché a memoria principal, para luego poder sobreescribir esta línea de la caché con la nueva transferencia de datos desde la memoria principal.

<p>Otra estrategia mas sencilla es la <b><i>escritura directa (write-through)</i></b>.
Cuando se realiza un requisito de escritura el nuevo dato es escrito en ambas memorias, en la caché y en la memoria principal. De esta manera, tanto la caché como la memoria principal tienen siempre copias validas de todos los datos. Aunque existe un rendimiento mas bajo que la estrategia anterior, muchas veces el procesador puede continuar ejecutando instrucciones mientras la escritura a memoria principal procede.



<p>Cuando se produce un fallo de caché, se obtiene una nueva línea de datos de la memoria principal.
En consecuencia, el procesador puede leer un byte de la memoria caché y, a continuación, el caché requiere una línea de, digamos, ocho bytes de la tienda principal. Como se puede imaginar, el costo de una falta en un acceso a caché conlleva una penalización adicional porque toda una línea tiene que ser llenada de memoria. Afortunadamente, las memorias modernas, las CPUs y los sistemas de antememoria soportan un modo de relleno de relleno en el que se puede transferir una ráfaga de elementos de datos consecutivos entre el almacén principal y la memoria caché. Veamos de nuevo los tiempos de acceso a la caché.

</section>
<section>
<header>Otras consideraciones en el diseño de cachés</header>

<p>Cuando se produce un fallo de caché, se obtiene una línea completa de datos desde la memoria principal.
Si el procesador necesita leer un unico byte y, debido fallo de caché, se requiere leer una línea de 8 bytes, 
podemos imaginar que el costo de un fallo en un acceso a caché conlleva una penalización adicional.
Afortunadamente, las tecnologías de memorias modernas y CPUs soportan un modo de 
completado llamado <b><i>burst-fill mode</i></b>, que puede transferir un bloque/ráfaga (busrt) de elementos de datos consecutivos entre la memoria principal y la memoria caché. 

<p>Otro aspecto de las memorias caché que debe tenerse en cuenta en los sistemas sofisticados es la <b><i>coherencia de la caché</i></b>. 
Como sabemos, los datos en el caché son copias de los datos en memoria principal. 
Cuando el procesador modifica los datos, debe modificar tanto la copia en el caché como la copia en memoria principal (aunque no necesariamente al mismo tiempo). 
Hay circunstancias en las que la existencia de dos copias (que pueden ser diferentes) del mismo elemento de datos causan problemas. Por ejemplo, un controlador de E/S que utiliza DMA podría intentar mover una línea antigua de datos de la memoria principal al disco, sin conocer que el procesador acaba de actualizar la copia de los datos en el caché (y que aún no ha actualizado en memoria principal). 
La coherencia del caché también se conoce como consistencia de datos.
En sistemas con varios procesadores es posible tener varias copias del mismo dato. Por ejemplo, una linea de datos de memoria principal podrías estar siendo utilizada por dos o más procesadores diferentes, los cuales cuentan con una cache individual cada uno. Si uno de los procesadores modifica una de las lineas de su caché, algún mecanismo debe estar presente para mantener la coherencia de las cachés.

<p>La organización de la memoria caché suele utilizar un <b><i>bit extra de validez (valid bit)</i></b> para cada entrada en la caché. El bit de validez es utilizado para verificar si la línea presente es válida o no. Cuando un procesador comienza a utilizar la caché (y por lo tanto la caché está vacía), todos los bits de validez están desactivados (por ejemplo, todos en cero), para indicar que todas las entradas son invalidas.
A medida que se comienzan a transferir datos de memoria principal a la caché, el bit de validez de cada línea de la caché se activa, para indicar que los datos son válidos. Cada vez que la caché verifica si la dirección de entrada está o no en la caché, tambien verifica el bit de validez.
<p>En algunas situaciones, varias líneas de la caché (o todas) deben ser invalidadida. Por ejemplo, en una operación de E/S. Supongamos por un momento que un controlador DMA está modificando lineas de memoria principal con datos provenientes de un dispositivo de entrada. Si existen copias en la caché de estos datos, el bit de validez puede ser utilizado para "invalidar" esa línea en la caché. Otro ejemplo común es el cambio de contexto de un proceso (si la computadora cuenta con un sistema operativo, o multiproceso). Previo a que otro proceso esté activo en la CPU, los bit de validez pueden utilizarse para invalidar toda la caché, ya que todas las lineas presentes pertenecen al proceso activo anterior.
</section>

<br>
<p>POR HACER:
<p>Politicas de reemplazo
<p>Emprolijar la terminología.
<p>Agregar los cuadros de notas importantes (obtenidos de los distintos libros)




</section>

</section>

<section class='nonum refs'>

<header>References</header>

<ol id=ref></ol>

</section>
</article>
<script>
function getText(e)
{
    var text = "";

    for (var x = e.firstChild; x != null; x = x.nextSibling)
    {
	if (x.nodeType == x.TEXT_NODE)
	{
	    text += x.data;
	}
	else if (x.nodeType == x.ELEMENT_NODE)
	{
	    text += getText(x);
	}
    }

    return text;
}

function getElementsByClassName(oElm, strTagName, strClassName){
	var arrElements = (strTagName == "*" && oElm.all)? oElm.all : oElm.getElementsByTagName(strTagName);
	var arrReturnElements = new Array();
	strClassName = strClassName.replace(/\-/g, "\\-");
	var oRegExp = new RegExp("(^|\\s)" + strClassName + "(\\s|$)");
	var oElement;
	for(var i=0; i<arrElements.length; i++){
		oElement = arrElements[i];
		if(oRegExp.test(oElement.className)){
			arrReturnElements.push(oElement);
		}
	}
	return (arrReturnElements)
}


function makeref()  // turn references into endnotes
{

// get element that will hold list of references

  var refcontainer = document.getElementById('ref');
  var ids = new Array();
  var ent = new Array();
  var refnum = 0;

// find all elements that contain index entries, go through them sequentially

//  ref = getElementsByClassName(document, "*", "ref");
  ref = document.getElementsByTagName("cite");
  for(var i=0; i < ref.length; i++) {
    ref[i].setAttribute("id", "ref"+i);

// store the reference in a string in an associative array

//    var str = getText(ref[i]);
    var str = ref[i].innerHTML;

// check to see if the entry is there already, if not add it

    var refnum = ent.indexOf(str);
    if (refnum < 0) {
      ent.push(str);
    } 
    refnum = ent.indexOf(str) + 1;

//  replace content of original element 

    ref[i].innerHTML = '[' + refnum + ']';
  }

// go through list of index entries, create one li element per entry

  for (var i=0; i<ent.length; i++) {
    var li = document.createElement("li");
    refnum=i+1;
    li.innerHTML = '['+refnum+'] '+ent[i];
    refcontainer.appendChild(li); 
  }
}

function maketf()    // make table footnotes
{

// get element that will hold list table footnotes

  var tfcontainer = document.getElementById('tf');
  var ids = new Array();
  var ent = new Array();
  var tfnum = 0;

// find all elements that contain index entries, go through them sequentially

  tf = getElementsByClassName(document, "*", "tablefoot");
  var tfstr="";
  for(var i=0; i < tf.length; i++) {
    tf[i].setAttribute("id", "tf"+i);

// store the reference in a string in an associative array

//    var str = getText(ref[i]);
    var str = tf[i].innerHTML;

// check to see if the entry is there already, if not add it

    var tfnum = ent.indexOf(str);
    if (tfnum < 0) {
      ent.push(str);
    } 
    tfnum = ent.indexOf(str) + 1;
    tfstr = tfstr + '*';

//  replace content of original element 

    tf[i].innerHTML = tfstr + ' ';
  }

// go through list of index entries, create one li element per entry

  tfstr = "";
  for (var i=0; i<ent.length; i++) {
    var li = document.createElement("li");
    tfnum=i+1;
    tfstr = tfstr + '*';
    li.innerHTML = tfstr+' '+ent[i];
    tfcontainer.appendChild(li); 
  }
}

</script>

</html>
﻿

