<html>
<style>
@import url(http://fonts.googleapis.com/css?family=PT+Serif:400,700,400italic,700italic);

/* Set up basic page design */

@page { 
  /* size: letter; */
  size: a4;
  margin: 1.1in;

  @footnotes {
    border-top: thin solid black; 
    border-clip: 100px;
    padding: 0;
    margin: 0.6em 0 0 0;
    padding: 0.3em 0 0 0;
  }
}

article { 
  /* columns: 2; */
  columns: 1;
  column-gap: 0.25in; 
  font: 11pt "PT Serif", serif;
  hyphens: auto;             /* turn on hyphenation */
  text-align: justify;       /* and justification */
  counter-reset: figure;
}

/* basic settings on commonly used elements */

html, body, div, header, p, blockquote, ul, ol, li, pre { margin: 0; padding: 0 }
li { margin-left: 1.5em }

header { 
  font: bold 12pt "PT Serif", serif; 
  margin: 1em 0 0.3em;
  page-break-after: avoid; break-after: avoid;
}

p { text-indent: 1.5em }
header + p { text-indent: 0 }
cite { font-style: normal }
pre { margin: 0.5em 0; padding: 0.3em; background: #eee }
ul { margin: 0.8em 0 }

/* sections-specific styling */

section.lead { 
  column-span: all; 
  text-align: center;
  margin: 2em 0;
  font-style: italic;
}

aside {
  background: #ddf;
  padding: 0.6em 1.3em 0.6em 1.3em; /* assuming it will end up on right page */
  font-style: italic;
  font-family: Lato, "Roboto Condensed", sans-serif;
  /* width: 75mm; */
  width: 100%;
  box-sizing: border-box;
  /* float: outside; */
  float: none;
  /* margin-outside: 180mm; */
  margin-outside: 180mm;
  text-align: inside;
  hyphens: manual;
}

section.lead h1 { font: bold 14pt "PT Serif", serif; text-align: center } /* used for article title */

section.lead .authors {
  display: table;
  margin: 0 auto; 
}

section.lead .vcard {
  display: table-cell;
  text-align: center;
  font: 11pt "PT Serif", serif;
}

section.lead a {
  display: block;
  padding: 0 1em;
  color: black;
  text-decoration: none;
}

section.abstract header {
  text-align: center;
}

section.refs p {
  margin: 0.5em 0;
  text-indent: 0;
}

section.refs .author { 
  font-variant: small-caps;
}

section.refs ol, table ol {
  margin: 0; padding: 0;
}

section.refs li, table li {
  list-style-type: none;
  margin-left: 1.5em;
  text-indent: -1.5em;
}

table { border-collapse: collapse; margin: 1em 0; width: 100%; }

table td {
  border: thin solid black;
  padding: 0.2em;
}

/* counters */

section:first-of-type { counter-reset: section }
section.nonum { counter-reset: none }

section { counter-increment: section }
section.nonum { counter-increment: none }

header:before { content: counters(section, ".") " " }
section.nonum header:before { content: none }

/* footnotes */

::footnote-call {
  content: "[" counter(footnote, lower-latin) "]";
  font-size: 83%;
  vertical-align: super;
  line-height: none
}

::footnote-marker {
  content: "[" counter(footnote, lower-latin) "]";
  list-style-position: inside;
  margin: 0; padding: 0 0.3em 0 0;
}

.foot {
  float: prince-column-footnote;
  font-size: 90%;
  footnote-style-position: outside;
  margin: 0.3em 0 0 1.3em; padding: 0; text-indent: 0;
}

/* page floats */

.column-top { float: column-top; margin-bottom: 2em }
.column-bottom { float: column-bottom;  margin-top: 2em }
.top { float: top;  margin-bottom: 2em }
.bottom { float: bottom;  margin-top: 2em }
top figcaption, .bottom figcaption { margin-left: 2em; margin-right: 2em }

figure { 
  counter-increment: figure;
  font-size: 0.9em;
/*  min-width: 300px; */
  max-width: 640px;
/*  min-height: 300px; */
  max-height: 480px;
  text-align: left;
  width: 90%;
  float: center;
  margin: 0.4in;
}

figcaption:before { 
  font-weight: bold;
  content: "Figure " counter(figure) ": " 
}

@media screen {
  body {
    margin: 3em;
  }
  article { 
    columns: 1;
    font: 16px/1.3 "PT Serif", serif;
    width: 100%;
  }
  .top, .bottom, .column-top, .column-bottom {
    float: right; 
    width: 55%;
    margin-right: -60%;
  }
  /* aside { float: none; width: auto; margin: 1em 0 } */
  aside { float: none; width: auto; margin: 0em 0em 0em 0em }
}

</style>

<body onload="makeref(); maketf();">
<article>


<section class=lead>
<h1>Memoria </h1>

<div class=authors>

<div class="vcard">
<i>
 <a class="url fn" href="http://www.pnr.iki.fi/cv.html">Rafael Ignacio Zurita</a>
 <a class="url org" href="http://www.aalto.fi">rafa@fi.uncoma.edu.ar</a>
</div>

<div class="vcard">
 <a class="url fn" href="http://www.pnr.iki.fi/cv.html">Rodolfo del Castillo</a>
 <a class="url org" href="http://www.usenix.org">rdc@fi.uncoma.edu.ar</a>
</i>
</div>

</div>

</section>

<section class='abstract nonum'>

<header>Resumen</header>

<!-- toc -->


</section>

<section>

<header> Introducción </header>
Para cualquier arquitectura de computadoras existen, por lo general, 
muchas implementaciones con diferentes precios y rendimiento.
El diseñador debe, en cada implementación, seleccionar una estructura
de computadora que cumpla con restricciones de costos y que, a su vez,
tenga el mejor rendimiento y confiabilidad posible. 
Esta estructura debe ser construida con tecnologías de componentes 
fácilmente disponibles y, tipicamente, debe poder ser modificable para
aceptar nuevas tecnologías.

<p><br><aside>This is a sample document to showcase page-based formatting. It contains a chapter from a <a href=http://en.wikibooks.org/>Wikibook</a> called <a href=http://en.wikibooks.org/wiki/Sensory_Systems>Sensory Systems</a>. None of the content has been changed in this article, but some content has been removed.
</aside>
<br>

<p>En la rápida evolución que han tenido las computadoras en las ultimas decadas,
el costo de la memoria ha sido el mayor factor en el precio general del 
sistema.
Aunque las memorias se hayan convertido tambien en componentes baratos
hay (y siempre habrán) tipos de memorias diferentes en cuanto
a rendimiento, capacidad de almacenamiento y costos.
<p>Debido a que la velocidad de la memoria tiene una influencia sustancial
en la velocidad de ejecución de la CPU, se tienen en cuenta diferentes tecnologías de memoria disponible al momento de diseñar la organización
general del sistema de memoria de un nuevo computador.
El objetivo de utilizar diferentes tipos de memoria en un mismo sistema es conseguir el rendimiento de una memoria de gran velocidad al coste de una memoria de baja velocidad, basándose en el principio de cercanía de referencias.
Esta organización se denomina <b><i>jerarquía de memoria de un computador</i></b>.

<p>Si el costo no es una restricción, el sistema de memoria entero podría
ser construido utilizando la memoria disponible mas rápida, como se hizo
para la computadora CRAY-1, y como suele hacerse en las supercomputadoras,
donde el costo no es un problema.
Para computadoras de bajo costo, esto es obviamente imposible.
De cualquier manera, una opción para computadoras personales y sistemas
embebidos es organizar el sistema de memoria dentro de una jerarquía
de niveles, donde cada nivel está compuesto de tecnologías de 
memorias diferentes (en cuanto a costo, capacidad y velocidad de acceso).
Esto puede hacerse debido al conocimiento existente, de las características
estadísticas de los patrones de acceso a memoria, que realizan 
programas típicos en ejecución. En la figura 1 se puede observar
un diagrama de esta jerarquía.

<!--figure class=column-top-->
<figure>
<a href="imagenes/Jerarquia_memoria.png"><img style="width: 100%" src="imagenes/Jerarquia_memoria.png"></a>
<figcaption>Jerarquía de memoria.</figcaption>
</figure>

<p> Un sistema de memoria típico en una computadora está compuesto de un 
número consecutivo de elementos de almacenamiento, usualmente bytes, en
donde cada uno tiene una única dirección física. Para leer el dato que
se encuentra en una ubicación de almacenamiento la CPU ubica la dirección
única en un registro interno en el bus de memoria (frecuentemente 
referido como el registro de dirección de memoria) y recive el 
dato que retorna en otro registro (el registro buffer de memoria), como
se muestra en la figura 11-2. Note que cada dirección univocamente
especifica una ubicación de memoria, por lo tanto, no se necesita
realizar una busqueda del dato.
</section>
<section>
<header> La aproximación de la tecnología mas rápida </header>

<p>Una manera de estructurar el sistema de memoria total es implementar un 
segmento de la memoria con la tecnología mas rápida disponible.
Ejemplificando, suponga que en un sistema existen 64Kbytes de memoria
más rápida posible, en la cual el usuario puede completar con datos y código
ejecutable. El usuario entonces ubicaría allí (al momento de cargar el programa) el código y los datos mas 
criticos, en cuanto a cantidad de accesos y tiempo de ejecución.
Esto le permite predecir el rendimiento y tiempo de ejecución con exactitud.
De cualquier manera, este método beneficia unicamente a sistemas
de propósitos específicos (ejemplo: sistemas embebidos), pero 
es improbable que beneficie a un sistema de tiempo compartido
(time-sharing system), en el cual la memoria física es asignada y reasignada
entre los usuarios durante distintos momentos (computadoras de propósito
general).
Otra desventaja de esta aproximación es que el programador debe
conocer la arquitectura de la memoria física para utilizarla correctamente.

<header> Memoria Caché </header>

<p>
La tecnología más común utilizada en una jerarquía de memoria de una computadora es la memoria caché (pronunciado "cash", del francés "cacher" que siginifica "ocultar/esconder").
Nota al pie de pagina: está oculta desde el punto de vista del programador, aparece como parte del espacio de memoria del sistema.

<p>La <b><i>memoria caché</i></b> es una memoria pequeña, costosa y de muy alta velocidad. Se encuentra ubicada junto a la CPU y mantiene las instrucciones y datos mas recientemente utilizados. Cuando un programa realiza un requisito a memoria, la CPU verifica primero si el dato está en la caché. Si está, entonces el dato es traído rapidamente sin necesidad de acceder la memoria principal. El objetivo primordial es reducir el tiempo medio de acceso a los datos de la memoria principal. En la figura 2 puede observarse una arquitectura que utiliza tres niveles de memoria caché.





<!--figure class=column-top-->
<figure>
<a href="imagenes/cachehierarchy2.svg"><embed style="width: 100%" src="imagenes/cachehierarchy2.svg"></a>
<figcaption>Esquema simplificado de una moderna arquitectura Intel x86, con cuatro nucleos y 3 niveles de caché dentro del chip del procesador.</figcaption>
</figure>


 <p>La memoria caché se puede comprender, en términos cotidianos, por su analogía con un agenda o cuaderno utilizado para registrar números de teléfono. Una guía telefónica completa (paginas blancas) contiene cientos de miles de números de teléfono, y nadie lleva un directorio telefónico todo el tiempo. Sin embargo, la mayoría de la gente tiene una agenda o cuaderno de notas con un centenar de números de teléfono de gente conocida. Aunque la fracción de todos los números de teléfono guardados en un cuaderno de alguien podría ser inferior al 0,01% del total de la guía telefónica, la probabilidad de que su próxima llamada sea a un número del cuaderno es muy alta. ¿Por qué? Porque la gente tiende a llamar a amigos y colegas con mucha frecuencia.

 <p>Un sistema de memoria con caché opera exactamente con el mismo principio del cuaderno de notas, encontrando la información que la CPU requiere frecuentemente en la memoria caché, en vez de en la memoria principal, la cual es  mucho más lenta. Desafortunadamente, a diferencia del cuaderno personal, la computadora no puede saber, a priori, qué datos serán posiblemente accedidos. Las caches de computadora funcionan según un principio de aprendizaje. Por experiencia aprenden qué datos se utilizan con más frecuencia y luego lo transfieren a la memoria caché.

 <p>La estructura general de una memoria caché se puede observar en la Figura 1. Un bloque de memoria caché se encuentra conectado a los buses de dirección y datos, en paralelo con la memoria principal mucho más grande. Tenga en cuenta que la afirmación de paralelo significa que los datos en la caché también se mantienen en la memoria principal. Para volver a la analogía con el cuaderno telefónico, escribir el número de un amigo en el cuaderno no elimina el número del directorio.

<!--figure class=column-top-->
<figure>
<a href="imagenes/cachehierarchy.svg"><embed style="width: 100%" src="imagenes/cachehierarchy.svg"></a>
<figcaption>Diagrama de bloques de una memoria caché general.</figcaption>
</figure>



La memoria caché requiere de un controlador para determinar si los datos que están siendo referenciados actualmente por la CPU residen en la memoria caché o si deben obtenerse de la memoria principal. Cuando el controlador de memoria caché obtiene la dirección del bus, se aplica la dirección actual al controlador de memoria caché, el controlador devuelve una señal llamada hit, que se afirma si los datos están actualmente en la memoria caché. Antes de ver cómo se organizan las memorias caché, demostraremos su efecto en el rendimiento de un sistema.


<p>El funcionamiento de la memoria caché se basa en el mismo principio que el
cuaderno con números de teléfono, ya que
el procesador no ejecuta instrucciones aleatoriamente. 
Debido a la naturaleza de los programas y sus estructuras de datos correspondientes, 
los datos requeridos por el procesador suelen estar muy agrupados en la memoria.
Frecuentemente se acceden en forma secuencial
(por ejemplo, cuando la CPU accede a las instrucciones del programa)
o desde zonas cercanas a direcciones recientemente
accedidas (cuado por ejemplo se procesa un arreglo o matrix de información).


<p>La observación del comportamiento de los programas muestra que las referencias a memoria realizadas
en un intervalo corto de tiempo tienden a utilizar una pequeña fracción
del total de la memoria.
Este comportamientno de acceso ha sido llamado <b><i>principio de localidad
de las referencias</i></b>[DENN68], o simplemente principio de localidad.
<p>Existen al menos dos tipos básicos de localidad: localidad espacial y temporal.
La localidad temporal se refiere a la reutilización de datos especificos,
dentro de un tiempo relativamente corto. Cuando un programa accede a la
ubicación en memoria de un dato o instrucción es bastante probable que vuelva a
acceder a la misma ubicación pronto. La localidad espacial, en cambio, se refiere
a la utilización de datos en ubicaciones de memoria cercanas a los elementos
accedidos recientemente. Si un programa accede a un dato o instruccion
en memoria es
altamente probable que tambien referencie a datos o instrucciones alojados
en direcciones proximas.

<p>Este fenómeno demostrado ha dado una base estadística para el diseño
de la estructura de la memoria de un computador basado en una jerarquía de memoria.

<p>En este caso, si la caché puede mantener suficientes datos para evitar
un gran número de referencias a la memoria principal, la velocidad
de ejecución incrementa substancialmente, y la utilizacion de la memoria
principal y del bus del sistema es también reducida beneficiosamente.
El objetivo es lograr una gran capacidad de memoria (esto se consigue
con memorias baratas y lentas) pero que puedan accederse con alta 
velocidad (a través de la utilización
de pequeñas memorias cachés entre la CPU y la memoria principal).

<section>
<header> Tiempo medio de acceso </header>


<p>El parámetro principal de un sistema de caché es su tasa de aciertos (hit ratio), h, que define la proporción de aciertos a todos los accesos. La tasa 
de aciertos es determinada a través de observaciones estadísticas del funcionamiento de un sistema real, y no puede calcularse facilmente. 

<p>Además, la proporción de aciertos depende de la naturaleza real de los programas que se están ejecutando. Es posible tener algunos programas con tasas de aciertos muy altas y otros muy bajas. Afortunadamente, el efecto de la localidad de las referencias provoca que la proporción de aciertos es, generalmente, del 98%. Antes de calcular el efecto de una memoria caché en el rendimiento de un procesador, necesitamos introducir algunos términos.


<p>Si un dato es leído o escrito k veces en un intervalo corto de tiempo
entonces la computadora necesita 1 referencia a memoria principal, y 
k - 1 referencias a memoria caché. Cuanto mas grande sea k mejor
será el rendimiento del sistema.
Es posible formalizar este cálculo utilizando el tiempo de acceso a caché,
que llamaremos c, y el tiempo de acceso a memoria principal, llamado aquí m.
h es la tasa de aciertos, la cual es la fracción de las referencias
que están disponibles en caché. 
<p> h = (k -1)/k. Con estas variables y definiciones el tiempo medio
de acceso = c + (1 - h) m
<p> Si h tiende a 1 entonces todas las referencias pueden
ser resueltas por la caché y el tiempo de acceso se aproxima a c. Por
otro lado, si h tiende a 0, cada referencia debe ser satisfecha
por la memoria principal y el tiempo de acceso se aproxima a c + m
(ya que para cada acceso primero se intenta resolver la referencia 
en la caché, pero al haber un fallo, se accede a memoria principal).
Una estrategia de mejora para este peor caso es realizar el acceso
a memoria principal en paralelo con la referencia a cache', pero 
esto requiere de mecanismos para descartar o detener la lectura
a memoria si ocurre un cache' hit.


<p>En la práctica la situación real no es tan simple como las ecuaciones sugieren.
Las computadoras son dispositivos sincronizados por relojes, y funcionan a la velocidad determinada por el reloj.
Consecuentemente, los accesos a memoria operan en uno o más ciclos de reloj. 
Suponga, hipotéticamente, que un procesador accede
a memoria principal en un ciclo de reloj, entonces agregando una caché no hará al sistema más rápido.



<p>El efecto del rendimiento de una memoria caché de una computadora depende de muchos factores, incluyendo la
manera en que la caché está organizada, y la forma en el cual los datos son escritos a memoria principal
cuando se realizan escrituras. 



<p> La importancia del tiempo de acceso a un dato que la CPU requiera puede verse reflejado en la tabla X.
En una computadora moderna (supongamos que contiene un procesaodr icore7, de arquitectura Intel X86) cada core puede ejecutar una instruccion cada 2nanosegundos.


<br>
<p><b>Ejemplo de tiempos de accesos a memoria en una arquitectura moderna</b>
<code>
<br>Core i7 Xeon 5500 Series Data Source Latency (approximate)               [Pg. 22]<br>
<br>
local  L1 CACHE hit,                              ~4 cycles (   2.1 -  1.2 ns )<br>
local  L2 CACHE hit,                             ~10 cycles (   5.3 -  3.0 ns )<br>
local  L3 CACHE hit, line unshared               ~40 cycles (  21.4 - 12.0 ns )<br>
local  L3 CACHE hit, shared line in another core ~65 cycles (  34.8 - 19.5 ns )<br>
local  L3 CACHE hit, modified in another core    ~75 cycles (  40.2 - 22.5 ns )<br>
<br>
remote L3 CACHE (Ref: Fig.1 [Pg. 5])        ~100-300 cycles ( 160.7 - 30.0 ns )<br>
<br>
local  DRAM                                                   ~60 ns<br>
remote DRAM                                                  ~100 ns<br>
<br><br></code>

<p> Si la caché puede almacenar suficienten datos para evitar un gran 
numero de referencias a memoria principal (lenta) entonces no sólo
la velocidad de ejecución incrementa, sino que tambien se reduce el uso
del bus del sistema y memoria.
</section>
<section>



<header>Organizacion interna de la memoria caché </header>
<p>
La estructura interna y el funcionamiento de una memoria caché 
describen dónde debe colocarse un bloque de memoria principal 
cuando se almacena en la caché. Las 3 organizaciones más utilizadas son 
caché con mapeo directo, caché asociativa, y caché asociativa por conjunto
de n vías.

<p>Directa: al bloque i-ésimo de memoria principal le corresponde la posición i módulo n, donde n es el número de bloques de la memoria caché. Cada bloque de la memoria principal tiene su posición en la caché y siempre en el mismo sitio. Su inconveniente es que cada bloque tiene asignada una posición fija en la memoria caché y ante continuas referencias a palabras de dos bloques con la misma localización en caché, hay continuos fallos habiendo sitio libre en la caché.
<p>Asociativa: Los bloques de la memoria principal se alojan en cualquier bloque de la memoria caché, comprobando solamente la etiqueta de todos y cada uno de los bloques para verificar acierto. Su principal inconveniente es la cantidad de comparaciones que realiza.
<p>Asociativa por conjuntos: Cada bloque de la memoria principal tiene asignado un conjunto de la caché, pero se puede ubicar en cualquiera de los bloques que pertenecen a dicho conjunto. Ello permite mayor flexibilidad que la correspondencia directa y menor cantidad de comparaciones que la totalmente asociativa.



<header>Organizacion interna de la memoria caché </header>

<p>Hay al menos tres maneras de organizar una memoria caché, cada una con su propio rendimiento y beneficios:
mapeado directo, asociativa, y caché asociativa por conjuntos.

<header>Caché de mapeo directo</header>



<p>La forma más sencilla de organizar una memoria caché es utilizar un mapeo directo, que se basa en un algoritmo simple: asignar el bloque de datos i de la memoria principal al bloque de datos i en la memoria caché. 


<p>La figura 4 ilustra la estructura de una memoria caché simple de mapeo directo. 
La memoria está compuesta de 32 palabras y se accede mediante un bus de direcciones de 5 bits que lo conecta a la CPU y caché.
El espacio de memoria ha sido divido en conjuntos y los conjuntos en líneas. 
<p>La caché contiene "entradas" llamadas comunmente líneas de caché (cache line). En este ejemplo particular cada línea de caché mantiene un bloque datos pequeño, formado por dos palabras consecutivas.

<p>Cada dirección es de 5 bits, y está compuesta por un campo de 2 bits para indicar el conjunto, un campo de 2 bits para indicar la línea de caché, y un campo de selección de palabra de 1 bit. La memoria caché tiene 4 líneas de dos palabras cada una. Cuando el procesador genera una dirección, se accede a la línea apropiada en la caché. Por ejemplo, si el procesador genera la dirección de 5 bits 01010, se accede a la línea 2 del conjunto 2.



<!--figure class=column-top-->
<figure>
<a href="imagenes/directo1.gif"><embed style="width: 70%" src="imagenes/directo1.gif"></a>
<figcaption>
La memoria caché simple de mapeo directo.</figcaption>
</figure>


 

<p>Un vistazo a la figura 4 revela que hay cuatro líneas número dos posibles: una línea 2 en el conjunto 0, una línea 2 en el conjunto 1, una línea 2 en el conjunto 2 y una línea 2 en el conjunto 3. En este ejemplo, el procesador accedió a la línea 2 en el conjunto 2. La pregunta obvia es, '¿cómo sabe el sistema si la línea 2 accedida en la caché es la línea 2 del conjunto 2 en la memoria principal?'

<p>La Figura 5 muestra que existe tambien una etiqueta (tag) asociada a cada línea en la memoria caché, que determina a qué conjunto pertenece esa línea. 
De esta manera es posible confirmar que la línea en la caché pertenece o no al conjunto especificado en la direcciión originada por el procesador.
Cuando el procesador accede a la línea 2, la etiqueta que pertenece a la línea 2 de la memoria caché se envía a un comparador. Al mismo tiempo, el campo del conjunto de la dirección originada por el procesador también se envía al comparador. Si son iguales, la línea en la caché es la línea deseada y se produce un acierto (hit). 

<!--figure class=column-top-->
<figure>
<a href="imagenes/directo2.gif"><embed style="width: 70%" src="imagenes/directo2.gif"></a>
<figcaption>
Resolución del conjunto de cada línea en una caché de mapeo directo.
</figure>

<p>Si las etiquetas (conjuntos) no son los iguales, se produce un fallo (miss) y se debe actualizar la memoria caché.
La antigua línea 2 del conjunto 1 es descartada o reescrita de nuevo a la memoria principal,
dependiendo de cómo está organizada la actualización de la memoria principal.


<p>La figura 6 muestra la estructura de un sistema de memoria caché de mapeo directo.
La caché es simplemente un bloque de memoria de lectura / escritura de acceso aleatorio de muy alta velocidad.
Tambien incorpora un comparador y una memoria rápida para las etiquetas.
El bus de direcciones contiene la dirección de entrada a la caché el cuál es utilizada para acceder a una única ubicación de línea de caché. La etiqueta en esa línea es entonces comparada con el valor del conjunto presente como parte de la dirección de entrada proveniente del bus de direcciones. Si la etiqueta concuerda con el número de conjunto de la dirección la señal de acierto (hit) es activada.

<p>Como muestra la Figura 6, la memoria de etiquetas de la caché no es más que una simple memoria de acceso aleatorio de alta velocidad con un comparador de datos incorporado. Algunos de los principales fabricantes de semiconductores han implementado esta memoria de etiquetas de caché de un solo chip.

<!--figure class=column-top-->
<figure>
<a href="imagenes/directo3.gif"><embed style="width: 70%" src="imagenes/directo3.gif"></a>
<figcaption>
Implementación de caché de mapeo directo.
</figure>


 

<p>La ventaja de la memoria caché de mapeo directo es casi evidente. Esta caché es un dispositivo 
ampliamente disponible que, aparte de su velocidad, no son más complejos que cualquier otro circuito integrado.
Además, esta caché no requiere ningún algoritmo de sustitución de línea. 
Si se accede a la línea x en el conjunto y se produce un fallo de caché, la línea x del conjunto y se carga de la memoria principal y se almacena en la línea x de la memoria caché. Por lo tanto, no hay mecanismos de decisión que deba seleccionar cuál línea de caché debe ser reemplazada cada vez que una nueva línea es cargada desde la memoria principal.

<p>Otra ventaja importante es su paralelismo inherente. Debido a que la caché y la memoria de etiquetas son independientes, ambas se pueden acceder simultáneamente. Una vez que la etiqueta ha coincidido y se ha producido un acierto, los datos presentados por la caché también serán válidos (suponiendo que la caché y la memoria de etiquetas tengan tiempos de acceso aproximadamente iguales).

<p>La desventaja de esta caché es casi un corolario de su ventaja. Un caché con n líneas tiene una restricción: en cualquier instante puede contener sólo una línea numerada x. Por lo tanto, no puede mantener una línea x del conjunto p y una línea x del conjunto q. Esta restricción existe porque hay un único bloque de datos en la caché para cada una de las lineas posibles. Para ejemplificar, observe el siguiente fragmento de código que llama a dos subrutinas en un bucle:

<br> <br>
<br> REPETIR
<br>      LLAMAR Get_data
<br>      LLAMAR Compare
<br> HASTA coincidencia o fin_de_los_datos
<br> <br> 


<p>Suponga que en la versión compilada de este código parte de la subrutina Get_data está en el conjunto x, línea y; y que parte de la subrutina Compare está en el conjunto z, línea y.
Debido a que una caché de mapeo directo puede contener sólo una línea y a la vez, la trama correspondiente a la línea y debe recargarse dos veces para cada conjunto a través del bucle.
El rendimiento de la caché para este caso es muy bajo (fallos de caché frecuentes).

<p>Supongamos ahora que una memoria caché de mapeo directo está casi vacía, y que la mayoría de sus líneas aún no se han cargado con datos. Sin embargo, las únicas líneas con datos válidos deben reemplazarse con frecuencia 
debido a que los accesos a memoria son direcciones con diferentes números de conjuntos pero misma línea.
Este caso también tendrá un rendimiento pobre (fallos de caché frecuentes). 
<p>A pesar de los ejemplos anteriores, mediciones estadísticas de programas reales indican que el rendimiento bajo del peor caso no tiene un impacto significativo en su rendimiento general.
Además, para ciertos casos, puede trabajar mejor que una caché totalmente asociativa, como veremos luego.
<p>La caché de mapeo directo es muy popular debido a su bajo costo de implementación y alta velocidad.

<p> CUADRO: de mejor caso que asociativa


</section>
<section>
<header>Caché asociativa</header>
<p>Una excelente forma de organizar una memoria caché que no presenta las limitaciones de la caché de mapeo 
directo se describe en la Figura 7, la cual se denomina memoria caché asociativa.
Este tipo de caché no tiene restricciones en cuanto a qué datos puede contener cada entrada.
En otras palabras, cada entrada en esta caché puede almacenar cualquier bloque (línea) de la memoria principal.

<p>Una dirección del procesador se divide en tres campos: la etiqueta, la línea y la palabra. 
Al igual que la memoria caché de mapeo directo, la unidad más pequeña de datos transferido 
dentro y fuera de la caché es la línea.
A diferencia de la caché de mapeo directo, no existe una relación
entre la ubicación de las líneas en la memoria principal con respecto a la ubicación
de las líneas en la caché.
La línea p en memoria principal se puede almacenar en la entrada q de la caché,
sin restricciones sobre los valores de p y q. 

<p>Como ejemplo, considere un sistema con 1 Mbyte de memoria principal y 64Kbytes
de caché asociativa. Si el tamaño de una línea es de cuatro palabras de 32 bits 
(es decir, 16 bytes), la memoria principal está compuesta de 220/16 = 64K líneas
y la caché está compuesta de 216/16 = 4096 líneas. 
Debido a que una caché asociativa permite que cada una de sus entradas
se cargue con cualquier línea de la memoria principal, cada entrada
puede contener cualquier de  
las 64K líneas posibles de la memoria principal. 
Para identificar de manera unívoca a qué línea de la memoria principal 
pertenecen las palabras almacenadas en cada entrada de la caché se utiliza,
en este caso, una etiqueta de 16bits asociada a cada entrada.

<!--figure class=column-top-->
<figure>
<a href="imagenes/asociativa1.gif"><embed style="width: 70%" src="imagenes/asociativa1.gif"></a>
<figcaption>
Caché asociativa.
</figure>


<p>Cuando el procesador genera una dirección, los bits de la misma seleccionan una ubicación tanto en la memoria principal como en la memoria caché.
Los bits que identifican la línea en la dirección no se pueden utilizar para direccionar 
una entrada en la memoria caché (a diferencia de la memoria caché de mapeo directa).
Como la caché asociativa puede almacenar cualquier línea de la memoria de 64K en una de sus
entradas, requiere una etiqueta de 16 bits (suponiendo que hay 216 líneas posibles
en la memoria principal) para cada una de sus entradas.
Más importante aún, ya que las entradas de la caché (es decir, líneas) no están ordenadas,
las etiquetas no están ordenadas y no se pueden almacenar en una tabla de búsqueda simple
como la memoria caché de mapeo directo.
En otras palabras, cuando la CPU requiere acceder a la línea i, el dato puede 
estar en cualquier entrada de la caché o puede no estar en la caché.

<p>La caché asociativa emplea un tipo especial de memoria llamada memoria asociativa.
Una memoria asociativa tiene una entrada de n bits (proveniente del bus de direccion),
 pero no necesariamente 2n ubicaciones
internas únicas. La entrada de dirección de n bits es una etiqueta que es
comparada con la etiqueta de cada una de sus entradas simultáneamente.

<p>Si la etiqueta de entrada coincide con una etiqueta almacenada en la caché,
los datos asociados con esa ubicación se presentan como salida. De lo contrario, la caché asociativa produce un fallo de caché (miss).
Una memoria asociativa no es direccionada de la misma manera que la memoria principal.
La memoria principal de la computadora requiere la dirección explícita de una localización, mientras que una memoria asociativa es accedida preguntando, "¿tiene este elemento almacenado en alguna parte?"

<p>Las memorias caché asociativas son eficientes porque la etiqueta que especifica la línea que se 
quiere obtener se compara simultáneamente con la etiqueta de cada entrada de la memoria caché.
En otras palabras, se accede a todas las ubicaciones al mismo tiempo. 
Desafortunadamente las memorias asociativas, aunque pequeñas, son muy costosas,
y cachés asociativas grandes no existen.
Además, una vez que la caché asociativa está llena, sólo se puede
introducir una nueva línea sobrescribiendo una existente.
Por lo tanto, las cachés asociativas deben utilizar alguna política de sustitución
para la entrada (línea) siendo descartada.

Las memorias caché totalmente asociativas no son prácticas, debido a su alto costo.
Sin embargo, la mayoría de las computadoras emplean un tipo de caché organizada 
de maneria mixta, utilizando algunos mecanismos de las cachés de mapeo directo y algunos de las cachés totalmente asociativas.
Este sistema combinado se llama caché asociativa por conjuntos.

Una caché asociativa por conjuntos está compuesta varias cachés de mapeo directo operados en paralelo.
La disposición más simple se denomina memoria caché asociativa de 2 vías y consta de dos 
memorias caché de mapeo directo. Cada línea en el sistema de caché está duplicado; Por ejemplo, hay dos líneas números 5 en la memoria caché. En consecuencia, ahora es posible almacenar dos líneas número 5, una línea 5 del conjunto x y una línea 5 del conjunto y.

La Figura 8 es una descripción de la caché asociativa mas comunmente utilizada, de 4 vías.
Cuando el procesador presenta una dirección de memoria a ser accedida la línea apropiada en cada una de las cuatro cachés de mapeo directo es accedida simultáneamente.
Debido a que hay cuatro líneas, un comparador sensicillo puede ser utilizado 
para determinar cuál (si presente) de las líneas en la caché es la apropiada para
recuperar los datos. En la figura 8, la salida de acierto (hit) de cada caché de mapeo directo
asignación directa es entrada a una puerta OR el cual genera un acierto (hit) is alguna de las
cachés generó un acierto.

<!--figure class=column-top-->
<figure>
<a href="imagenes/asociativaporconjuntos.gif"><embed style="width: 70%" src="imagenes/asociativaporconjuntos.gif"></a>
<figcaption>
Caché asociativa por conjuntos de 4 vías.
</figure>

</section>
<p>Agregar del patterson lo de chequear las entradas o como funciona.

<p>Luego las politicas de reemplazo y que pasa con las escrituras.

<p>Emprolijar la terminología.

<p>COmentar cuando se invalida la caché.




</section>

</section>

<section class='nonum refs'>

<header>References</header>

<ol id=ref></ol>

</section>
</article>
<script>
function getText(e)
{
    var text = "";

    for (var x = e.firstChild; x != null; x = x.nextSibling)
    {
	if (x.nodeType == x.TEXT_NODE)
	{
	    text += x.data;
	}
	else if (x.nodeType == x.ELEMENT_NODE)
	{
	    text += getText(x);
	}
    }

    return text;
}

function getElementsByClassName(oElm, strTagName, strClassName){
	var arrElements = (strTagName == "*" && oElm.all)? oElm.all : oElm.getElementsByTagName(strTagName);
	var arrReturnElements = new Array();
	strClassName = strClassName.replace(/\-/g, "\\-");
	var oRegExp = new RegExp("(^|\\s)" + strClassName + "(\\s|$)");
	var oElement;
	for(var i=0; i<arrElements.length; i++){
		oElement = arrElements[i];
		if(oRegExp.test(oElement.className)){
			arrReturnElements.push(oElement);
		}
	}
	return (arrReturnElements)
}


function makeref()  // turn references into endnotes
{

// get element that will hold list of references

  var refcontainer = document.getElementById('ref');
  var ids = new Array();
  var ent = new Array();
  var refnum = 0;

// find all elements that contain index entries, go through them sequentially

//  ref = getElementsByClassName(document, "*", "ref");
  ref = document.getElementsByTagName("cite");
  for(var i=0; i < ref.length; i++) {
    ref[i].setAttribute("id", "ref"+i);

// store the reference in a string in an associative array

//    var str = getText(ref[i]);
    var str = ref[i].innerHTML;

// check to see if the entry is there already, if not add it

    var refnum = ent.indexOf(str);
    if (refnum < 0) {
      ent.push(str);
    } 
    refnum = ent.indexOf(str) + 1;

//  replace content of original element 

    ref[i].innerHTML = '[' + refnum + ']';
  }

// go through list of index entries, create one li element per entry

  for (var i=0; i<ent.length; i++) {
    var li = document.createElement("li");
    refnum=i+1;
    li.innerHTML = '['+refnum+'] '+ent[i];
    refcontainer.appendChild(li); 
  }
}

function maketf()    // make table footnotes
{

// get element that will hold list table footnotes

  var tfcontainer = document.getElementById('tf');
  var ids = new Array();
  var ent = new Array();
  var tfnum = 0;

// find all elements that contain index entries, go through them sequentially

  tf = getElementsByClassName(document, "*", "tablefoot");
  var tfstr="";
  for(var i=0; i < tf.length; i++) {
    tf[i].setAttribute("id", "tf"+i);

// store the reference in a string in an associative array

//    var str = getText(ref[i]);
    var str = tf[i].innerHTML;

// check to see if the entry is there already, if not add it

    var tfnum = ent.indexOf(str);
    if (tfnum < 0) {
      ent.push(str);
    } 
    tfnum = ent.indexOf(str) + 1;
    tfstr = tfstr + '*';

//  replace content of original element 

    tf[i].innerHTML = tfstr + ' ';
  }

// go through list of index entries, create one li element per entry

  tfstr = "";
  for (var i=0; i<ent.length; i++) {
    var li = document.createElement("li");
    tfnum=i+1;
    tfstr = tfstr + '*';
    li.innerHTML = tfstr+' '+ent[i];
    tfcontainer.appendChild(li); 
  }
}

</script>

</html>
﻿

